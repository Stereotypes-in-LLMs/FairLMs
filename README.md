# FairLMs

This repository contatains the source code of the paper {name}.

## Overview

This project studies gender bias in large language models (LLMs) within the Ukrainian language, focusing on the use of femininitives and the hiring domain. We propose a benchmark dataset and evaluate multiple debiasing strategies, including:

- Prompt-based debiasing
- Embedding debiasing
- Fine-tuning with LoRA

We introduce both Q&A-based and probability-based evaluation metrics to quantify bias and model performance.

## Repository Structure

```bash
FairLMs/
â”œâ”€â”€ configs/       # Configuration files for running experiments
â”œâ”€â”€ data/          # Synthetic hiring dataset (351 professions Ã— 8 variants)
â”œâ”€â”€ debias/        # Implementations of prompt, embedding, and fine-tuning methods
â”œâ”€â”€ metrics/       # Evaluation metrics (QAAccMetric, ProbDiffMetric, etc.)
â”œâ”€â”€ parser/        # Wrapper for running debiasing methods
â”œâ”€â”€ results/       # Saved resulting metrics 
â”œâ”€â”€ utils.py       # Utility functions
â”œâ”€â”€ main.py        # Main script for running experiments
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Dataset
The dataset includes:

Two datasets for accuracy and probability measurment

351 Ukrainian professions

8 variations per profession:

- Male / Female

- Femininitive / Non-femininitive

- Relevant / Irrelevant experience

ğŸ“ See data/ for full samples and construction logic.

## Evaluation Metrics

Implemented in `metrics/`:

- `QAAccMetric`: QA accuracy (F1) per gender/form

- `QADiffMetric`: Prediction consistency across gendered variants

- `ProbAccMetric`: Classification accuracy via probability

- `ProbDiffMetric`: Distributional bias via likelihood scores

Results are logged in `results/`.
